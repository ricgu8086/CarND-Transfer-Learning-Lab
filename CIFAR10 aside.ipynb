{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to load the trained model on the Keras Lab for the German Traffic Signs dataset we would have to do the following:\n",
    "\n",
    "```python\n",
    "path2json = \"/home/ubuntu/SDC/CarND-Keras-Lab/final_trained_model.json\"\n",
    "path2weights = \"/home/ubuntu/SDC/CarND-Keras-Lab/final_trained_model.pkl\"\n",
    "\n",
    "# Load architecture\n",
    "with open(path2json, \"r\") as fin:\n",
    "    model_json = fin.readlines()\n",
    "    \n",
    "model = model_from_json(model_json[0])\n",
    "\n",
    "# Load weights\n",
    "with open(path2weights, \"rb\") as fin:\n",
    "    model_weights = pickle.load(fin)\n",
    "    \n",
    "model.set_weights(model_weights)\n",
    "```\n",
    "\n",
    "However, for this exercise (regarding we are not going to do transfer learning) is easier to define the architecture from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "# Normalization\n",
    "def normalize(x, a, b):\n",
    "    return a + (x-x.min())*(b-a)/(x.max() - x.min())\n",
    "\n",
    "a, b = -0.5, 0.5\n",
    "X_train_normalized = normalize(X_train, a, b)\n",
    "X_test_normalized = normalize(X_test, a, b)\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train, nb_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, nb_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, border_mode='valid', input_shape=(32, 32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(nb_filter=64, nb_row=3, nb_col=3, border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10)) # In CIFAR10 there are 10 classes instead of 43 in the German Traffic Signs dataset\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile('adam', 'categorical_crossentropy', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 13s - loss: 1.7524 - acc: 0.3452 - val_loss: 1.3496 - val_acc: 0.5097\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 11s - loss: 1.3947 - acc: 0.4994 - val_loss: 1.1662 - val_acc: 0.5843\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 11s - loss: 1.2568 - acc: 0.5526 - val_loss: 1.0974 - val_acc: 0.6152\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 12s - loss: 1.1515 - acc: 0.5942 - val_loss: 1.0019 - val_acc: 0.6517\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 13s - loss: 1.0621 - acc: 0.6318 - val_loss: 0.9556 - val_acc: 0.6683\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 13s - loss: 1.0038 - acc: 0.6555 - val_loss: 0.9242 - val_acc: 0.6746\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 12s - loss: 0.9494 - acc: 0.6746 - val_loss: 0.8973 - val_acc: 0.6921\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 12s - loss: 0.9131 - acc: 0.6852 - val_loss: 0.8918 - val_acc: 0.6879\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 12s - loss: 0.8626 - acc: 0.7047 - val_loss: 0.8765 - val_acc: 0.6996\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 11s - loss: 0.8348 - acc: 0.7146 - val_loss: 0.8819 - val_acc: 0.7002\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_normalized, y_train_one_hot, batch_size=64, nb_epoch=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would it take less doubling batch_size? And what about accuracy?\n",
    "\n",
    "nvidia-smi command said the GPU utilization is only 52%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 9s - loss: 0.5801 - acc: 0.7984 - val_loss: 0.8714 - val_acc: 0.7186\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 9s - loss: 0.5756 - acc: 0.8008 - val_loss: 0.8825 - val_acc: 0.7214\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 9s - loss: 0.5571 - acc: 0.8064 - val_loss: 0.8687 - val_acc: 0.7217\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 8s - loss: 0.5380 - acc: 0.8100 - val_loss: 0.9039 - val_acc: 0.7232\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 7s - loss: 0.5333 - acc: 0.8139 - val_loss: 0.9120 - val_acc: 0.7210\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 8s - loss: 0.5198 - acc: 0.8189 - val_loss: 0.9318 - val_acc: 0.7175\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 8s - loss: 0.5229 - acc: 0.8155 - val_loss: 0.9337 - val_acc: 0.7159\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 8s - loss: 0.5037 - acc: 0.8233 - val_loss: 0.9259 - val_acc: 0.7200\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 8s - loss: 0.4964 - acc: 0.8278 - val_loss: 0.9263 - val_acc: 0.7196\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 8s - loss: 0.4884 - acc: 0.8290 - val_loss: 0.9368 - val_acc: 0.7191\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_normalized, y_train_one_hot, batch_size=128, nb_epoch=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now utilization is 61%. It's a bit faster in training and the val_acc is more steep, but at the end it reaches a similar accuracy (in train has greatly improved). My intuition is that a learning rate schedule could help to improved the results after epoch 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test_normalized, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97403912444114682, 0.71750000000000003]\n",
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "print(res)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, not bad for a baseline. In the German Traffic Signs dataset this architecture leaded to these results:\n",
    "\n",
    "```python\n",
    "print(res)\n",
    "print(model.metrics_names)\n",
    "[0.16229501442625374, 0.95558194779066086]\n",
    "['loss', 'acc']\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
